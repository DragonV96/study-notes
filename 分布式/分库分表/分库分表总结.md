# 分库分表总结

  * [1 面试题](#1-%E9%9D%A2%E8%AF%95%E9%A2%98)
    * [1\.1 为什么要分库分表](#11-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8)
    * [1\.2 如何动态切换至分库分表系统](#12-%E5%A6%82%E4%BD%95%E5%8A%A8%E6%80%81%E5%88%87%E6%8D%A2%E8%87%B3%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E7%B3%BB%E7%BB%9F)
    * [1\.3 如何设计可以动态扩容缩容的分库分表方案](#13-%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%8F%AF%E4%BB%A5%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9%E7%BC%A9%E5%AE%B9%E7%9A%84%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E6%96%B9%E6%A1%88)
* [1\.4 分库分表后的 id 主键如何处理](#14-%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%90%8E%E7%9A%84-id-%E4%B8%BB%E9%94%AE%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86)

## 1 面试题

### 1.1 为什么要分库分表

**1. 为什么要分库分表？**

- 分表：单表数据达到几百万时，sql 的执行性能相对开始较差了，当其超过千万数据时，sql 的执行性能急剧下降，为了最大限度的保持 sql 的执行性能，必须分表；
- 分库：一个库的并发最多支撑到2000，超过则宕机崩溃，而一个健康的单库并发大概在每秒1000左右。

|              | 分库分表前                   | 分库分表后                                   |
| ------------ | ---------------------------- | -------------------------------------------- |
| 并发支撑情况 | MySQL 单机部署，扛不住高并发 | MySQL从单机到多机，能承受的并发增加了多倍    |
| 磁盘使用情况 | MySQL 单机磁盘容量几乎撑满   | 拆分为多个库，数据库服务器磁盘使用率大大降低 |
| SQL 执行性能 | 单表数据量太大，SQL 越跑越慢 | 单表数据量减少，SQL 执行效率明显提升         |



**2. 用过哪些分库分表中间件？说一下它们分别有什么优缺点？**

常见的分库分表中间件有：

- Cobar
- TDDL
- Atlas
- Sharding-jdbc
- Mycat

1）Cobar

Cobar 属于 Proxy 层方案，介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 Cobar 集群，Cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。

- 缺点：
    - ①不支持读写分离、存储过程、跨库 join 和分页等操作；
    - ②社区不活跃，框架几年未更新

> 阿里 b2b 团队开源的框架，近几年没更新，差不多算是被抛弃的状态了。

2）TDDL

TDDL 属于 client 层方案。

- 优点：
    - 支持基本的 crud 语法和读写分离
- 缺点：
    - ①不支持 join、多表查询等语法；
    - ②依赖淘宝的 diamond 配置管理系统

> 由淘宝团队开源的框架。

3）Atlas

Atlas 属于 proxy 层方案。

- 缺点：社区不活跃，框架几年未更新

> 360开源的框架。

4）Sharding-jdbc

Sharding-jdbc 属于 client 层方案，是[ `ShardingSphere` ](https://shardingsphere.apache.org/)的 client 层方案，[ `ShardingSphere` ](https://shardingsphere.apache.org/)还提供 proxy 层的方案 Sharding-Proxy。

- 优点：
    - ①SQL 语法支持比较多，没有太多限制
    - ②社区活跃，框架正在开发和维护
    - ③ 4.0.0-RC1版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力通知事务、TCC 事务）
    - ④不用部署，运维成本低，无需代理曾的二次转发请求，性能高
- 缺点：
    - 耦合度高，项目升级成本高（每个系统都需要引入 Sharding-jdbc 的依赖）

> 当当开源的框架。

5）Mycat

Mycat 属于 proxy 层方案（基于 Cobar 改造的），是[ `ShardingSphere` ](https://shardingsphere.apache.org/)的 client 层方案，[ `ShardingSphere` ](https://shardingsphere.apache.org/)还提供 proxy 层的方案 Sharding-Proxy。

- 优点：
    - ①支持的功能完善
    - ②社区活跃，框架正在开发和维护
    - ③耦合度低，项目升级成本低，只需升级 MyCat 所在服务
- 缺点：
    - ①相比 Sharding-jdbc 经历的锤炼较少
    - ②需要部署，运维成本高

6）总结：

- Sharding-jdbc 适用于中小型公司，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多
- Mycat 适用于中大型公司，proxy 层方案耦合度低，大公司系统和项目非常多，团队很大，人员充足



**3. 具体是如何对数据库如何进行水平拆分或垂直拆分的？**

- 水平拆分：把一个表的数据分散到多个库的多个表里去，但每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据

    > 目的：①将数据均匀放更多的库里，然后用多个库来扛更高的并发；②用多个库的存储容量来进行扩容。

- 垂直拆分：把一个较多字段的表拆分成多个表**，**或多个库上去。每个库表的结构都不一样，每个库表都包含部分字段

    > 目的：将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。（因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好）

- 表层面的拆分：将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能

    > 分库分表之后，中间件可以根据指定的某个字段值，比如说 userid，自动路由到对应的库上，然后再自动路由到对应的表里。

还有两种业务相关的拆分方式：

①按照数据范围水平拆分：每个库分一段连续的数据

- 优点：扩容的简单方便（只要预备好，定时准备一个库就可以了，到下个月份的时候，直接写新库）
- 缺点：大部分的请求，都是访问最新的数据

> 一般是按比如时间范围分，但是这种较少用，因为很容易产生热点问题，大量的流量都打在最新的数据上了。

②按某个字段 hash 一下均匀分散**（常用）**

- 优点：可以平均分配每个库的数据量和请求压力
- 缺点：扩容起来比较麻烦，会有一个数据迁移的过程（之前的数据需要重新计算 hash 值重新分配到不同的库或表）

### 1.2 如何动态切换至分库分表系统

**现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？**

有两种方案：

- 停机迁移方案
- 双写迁移方案

1）停机迁移方案

最 low 的方案，但是很简单，流程如下：

①发布公告，声明系统停机升级，如0 点到早上 6 点进行运维，无法访问；

②0 点停机，系统停掉，没有流量写入了，此时老的单库单表数据库静止了，将提前写好的**导数的一次性工具**（如 Kettle 脚本）直接跑起来，然后将单库单表的数据读出来，写到分库分表好的库里去；

③导完数据后，修改系统中各个服务的数据库连接配置，可能包括代码和 SQL，直连最新的库上；

④验证

2）双写迁移方案

常用的迁移方案，不用停机，流程如下：

①在线上系统里面，之前所有写库的地方，增删改操作，**除了对老库增删改，都加上对新库的增删改**，这就是所谓的**双写**，同时写俩库，老库和新库；

②**系统部署**后，新库数据差别较大，用提前写好的导数据工具的脚本（如 Kettle），跑起来读老库数据写新库，写的时候要根据 gmt_modified 这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写；（即不允许用老数据覆盖新数据）

③导完一轮数据后，仍然可能存在不一致的数据，再用脚本自动做一轮校验，对比新老库每个表的每条数据，如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止；

④当数据完全一致时，基于仅使用分库分表的最新代码，重新部署一次即可。

### 1.3 如何设计可以动态扩容缩容的分库分表方案

**如何设计可以动态扩容缩容的分库分表方案？**

有两种方案：

- 停机扩容方案
- 优化后的方案

1）停机扩容方案

步骤同本文档 1.2 章节的停机迁移方案，唯一不同的就是导数据工具的脚本，是把现有库表的数据抽出来慢慢导入到新的库和表里去。（数据量巨大的情况下，时间比较久，工作量巨大，严重时会耽误项目进度，用户流失，可行性较低）

2）优化后的方案

分 32 个库，每个库 32 个表，那么总共是 1024 张表。

每个库正常承载的写入并发量是 1000，那么 32 个库就可以承载 32 * 1000 = 32000 的写并发，如果每个库承载 1500 的写并发，32 * 1500 = 48000 的写并发，接近 5 万每秒的写入并发，前面再加一个MQ，削峰，每秒写入 MQ 8 万条数据，每秒消费 5 万条数据。

1024 张表，假设每个表放 500 万数据，在 MySQL 里可以放 50 亿条数据。

每秒 5 万的写并发，总共 50 亿条数据。

方案的具体方式：

①利用 `32 * 32` 来分库分表，即分为 32 个库，每个库里一个表分为 32 张表。一共就是 1024 张表。某个 id 先根据 32 取模路由到库，再根据 32 取模路由到库里的表。

| orderId | id % 32 (库) | id / 32 % 32 (表) |
| ------- | ------------ | ----------------- |
| 259     | 3            | 8                 |
| 1189    | 5            | 5                 |
| 352     | 0            | 11                |
| 4593    | 17           | 15                |

②刚开始这个库可能就是逻辑库，建在一个数据库上的，就是一个 MySQL 服务器可能建了 n 个库，比如 32 个库。后面如果要拆分，就是不断在库和 MySQL 服务器之间做迁移就可以了。然后系统配合改一下配置即可。

> 比如说最多可以扩展到 32 个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到 1024 个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是 1024 个表。
>
> 哪怕是要减少库的数量，也很简单，就是按倍数缩容就可以了，然后修改一下路由规则。

流程总结：

①设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32 库 * 32 表；

②路由的规则，如 orderId 模 32 = 库，orderId / 32 模 32 = 表；

③扩容的时候，申请增加更多的数据库服务器，装好 MySQL，呈倍数扩容，如 4 台服务器，扩到 8 台服务器，再到 16 台服务器；

④由 DBA 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，迁移工具如 Kettle；

⑤修改一下新库配置，调整迁移的库所在数据库服务器的地址；

⑥重新发布系统，上线，原先的路由规则完全不用变，直接可以基于 n 倍的数据库服务器的资源，继续进行线上系统的提供服务。

### 1.4 分库分表后的 id 主键如何处理

**分库分表之后，id 主键如何处理？**

有五种方案：

- 数据库自增 id
- 设置数据库 sequence 或者表自增字段步长
- UUID
- 获取系统当前时间
- snowflake 算法

1）数据库自增 id

- 优点：方便简单
- 缺点：单库生成自增 id，高并发情况下有瓶颈
- 适合场景：**并发不高，但是数据量太大**导致的分库分表扩容

2）设置数据库 sequence 或者表自增字段步长

- 优点：可以防止产生重复 id；实现简单；性能较高
- 缺点：服务节点固定，步长也固定，增加服务节点时改动工作量大
- 适合场景：id 不能重复的场景

3）UUID

- 优点：本地生成，无需数据库
- 缺点：UUID 太长占用空间大，**作为主键性能比较差**
- 适合场景：随机生成个什么文件名、编号之类的场景

> UUID 不具有顺序性，会导致 B+ 树索引在写的时候有过多的随机写操作（连续的 ID 可以产生部分顺序写）由于在写的时候不能产生有顺序的 append 操作，而需要进行 insert 操作，将会读取整个 B+ 树节点到内存，在插入这条记录后会将整个节点写回磁盘，这种操作在记录占用空间比较大的情况下，性能下降明显。

4）获取系统当前时间

- 缺点：并发很高时，会出现重复的情况（比如一秒并发几千）
- 适合场景：将当前时间跟很多其他的业务字段拼接起来，作为一个 id 的场景

5）snowflake 算法

生成的 id 结构如下：

```
 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000
 |			|				   |		  |
1bit符号位		41bit时间戳			10bit机器id	12bit序列号
```

- 1`bit`：第一位符号位，固定为0，表示正数
- 41`bit`：41位表示毫秒级时间戳**【可以表示（1L<<41）/(1000L * 3600 * 24 * 365)=69年的时间】**
- 10`bit`：10位机器id可以分别表示1024台机器**【如果对机房划分有需求，还可以将 10-bit 分 5-bit 给机房，分 5-bit 给工作机器。这样就可以表示32个机房，每个机房下可以有32台机器，可以根据自身需求定义】**
- 12`bit`：最后12位表示序列号**【每台机器每毫秒内最多生成2^12即4096个序列号】**

理论上 snowflake 方案的 QPS 约为 409.6w/s，这种分配方式可以保证在任何一个机房的任何一台机器在任意毫秒内生成的 ID 都是不同的。

> 官方给出数据为每台机器至少每秒生成10k条数据，且响应时间在2毫秒以内。

- 优点：有序单调递增，且全局唯一；高性能，低延迟；不依赖第三方系统，以服务的方式部署，稳定性高；可根据自身业务特性分配bit位，非常灵活
- 缺点：强依赖于机器的时间，一旦时间回拨则无法保证id唯一性
- 适用场景：分布式系统中高并发的场景

